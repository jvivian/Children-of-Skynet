{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Keras\n",
    "\n",
    "This notebook uses Keras, a high-level neural network system, to build a model capable of identifying hand-written digits (0 - 9) with high accuracy.\n",
    "\n",
    "This MNIST database contains 70,000 pictures of digits. Each picture is stored as a matrix of 28 x 28 pixels, whose values represent how dark the pixel is on a scale of 0 to 255. We'll use 60,000 of these samples to train the model, then evaluate its accuracy by testing it on remaining 10,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our model is a sequential stack of CNN layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# \"Core\" layers of Keras\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# CNN layers\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "# Datasets and Utils\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Numpy and Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train (capital X for matrix) contains our 60,000 train samples. <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "60,000 samples that are 28 x 28 pixels each. Let's plot one as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD0CAYAAACo2tvDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13fa70ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocess Data\n",
    "\n",
    "Tensorflow requires we declare a dimension for the _depth_ of the input image. A full-color image with 3 **RGB** channels would have a depth of 3. Our black and white pixel image has a depth of 1.\n",
    "\n",
    "Transform the dataset from (n, width, height) to (n, width, height, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_samples, img_rows, img_cols = X_train.shape\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# Input shape of a single image\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n"
     ]
    }
   ],
   "source": [
    "# Declare the types for our inputs\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize dataset to be between 0 and 1. MNIST uses 255 as a maximum value\n",
    "print np.max(X_train)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print y_train.shape\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in a typical format for machine learning. It is a list of labels, one for each samples in X_train. Keras requires we reshape the data into 10 distinct classes, one for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row corresponds to '5', the next row to '0', and the next row to '4', which matches our first array. This is typically called \"One Hot Encoding\", where _categorical_ features are turned into binary vectors of 0's and 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Convolution2D(filters=32, \n",
    "                        kernel_size=(3, 3), \n",
    "                        activation='relu', \n",
    "                        input_shape=(28,28, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters is an integer that represents the dimensionality of the output space. The kernel_size specifies the width and height of the 2D convolution window.\n",
    "\n",
    "Add more layers to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jvivian/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dropout** layer is an important method for regularizing the model to prevent overfitting. Explanation [here](https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network).\n",
    "\n",
    "MaxPooling2D is a way to reduce the number of parameters in our model by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values in the 2x2 filter.\n",
    "\n",
    "So far, for model parameters, we've added two Convolution layers. To complete our model architecture, let's add a fully connected layer and then the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dense layers, the first parameter is the output size of the layer. Keras automatically handles the connections between layers.\n",
    "\n",
    "Note that the final layer has an output size of 10, corresponding to the 10 classes of digits.\n",
    "\n",
    "Also note that the weights from the Convolution layers must be flattened (made 1-dimensional) before passing them to the fully connected Dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model\n",
    "\n",
    "Compiling the model just means specifying a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 145s - loss: 0.2525 - acc: 0.9238 - val_loss: 0.0566 - val_acc: 0.9806\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 135s - loss: 0.0968 - acc: 0.9706 - val_loss: 0.0415 - val_acc: 0.9855\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 133s - loss: 0.0736 - acc: 0.9780 - val_loss: 0.0370 - val_acc: 0.9879\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 147s - loss: 0.0610 - acc: 0.9812 - val_loss: 0.0346 - val_acc: 0.9882\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 131s - loss: 0.0544 - acc: 0.9833 - val_loss: 0.0310 - val_acc: 0.9897\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 140s - loss: 0.0457 - acc: 0.9859 - val_loss: 0.0314 - val_acc: 0.9901\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 137s - loss: 0.0433 - acc: 0.9862 - val_loss: 0.0324 - val_acc: 0.9897\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 136s - loss: 0.0370 - acc: 0.9883 - val_loss: 0.0302 - val_acc: 0.9909\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 135s - loss: 0.0352 - acc: 0.9887 - val_loss: 0.0298 - val_acc: 0.9921\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 136s - loss: 0.0324 - acc: 0.9898 - val_loss: 0.0295 - val_acc: 0.9914\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 136s - loss: 0.0308 - acc: 0.9901 - val_loss: 0.0282 - val_acc: 0.9914\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 137s - loss: 0.0282 - acc: 0.9907 - val_loss: 0.0290 - val_acc: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14eecaf90>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, \n",
    "          batch_size=128, \n",
    "          epochs=12, \n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss:', 0.029)\n",
      "('Test accuracy:', 0.9915)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', round(score[0], 4))\n",
    "print('Test accuracy:', round(score[1], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closer Look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what is being evaluated by **Test Accuracy**, let's look at a single example from our X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "print y_test[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 50th sample in our test set is for the number 6. Let's see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD0CAYAAACo2tvDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13faae190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Because our dimensions are 60.000, 28, 28, and 1, we select the 50th sample as follows:\n",
    "# 50 for the 50th sample. \":\" means all values, so all values for width and height, then 0 at the end for that dim.\n",
    "plt.imshow(X_test[50,:,:,0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging this sample into our model, it will give us confidence scores for how likely it thinks each number is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 0\t Probability: 3.6093805722e-09\n",
      "Digit: 1\t Probability: 3.45599200299e-13\n",
      "Digit: 2\t Probability: 2.7453649163e-12\n",
      "Digit: 3\t Probability: 2.5316632471e-12\n",
      "Digit: 4\t Probability: 4.57789986474e-11\n",
      "Digit: 5\t Probability: 2.28169909633e-08\n",
      "Digit: 6\t Probability: 1.0\n",
      "Digit: 7\t Probability: 1.76142383217e-18\n",
      "Digit: 8\t Probability: 1.28835608848e-10\n",
      "Digit: 9\t Probability: 2.94866724353e-13\n"
     ]
    }
   ],
   "source": [
    "# I use the clunky syntax [50:51] to select a single sample but retain the dimensionality\n",
    "# X_test[50] returns a (28 x 28 x 1) vector but our model requires an (n x 28 x 28 x 1) tensor\n",
    "for i, j in zip(xrange(10), model.predict(X_test[50:51])[0]):\n",
    "    print 'Digit: {}\\t Probability: {}'.format(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dope!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
